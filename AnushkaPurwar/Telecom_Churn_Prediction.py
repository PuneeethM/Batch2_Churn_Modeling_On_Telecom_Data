# -*- coding: utf-8 -*-
"""Anushka_TelecomChurnPrediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZycV74GP_j3DFzb8AW1-DEYCf5hNh7JF

# **Telecom Churn Prediction**
"""

import pandas as pd
import numpy as np

df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/TelecomChurnPrediction/Churn_ Data.csv')
df.head(5)

"""# **Data Preprocessing**"""

df.info()

df.shape

#data types of the columns
df.dtypes

df.isna().sum()

df.duplicated().sum()

uni_col=df.nunique() #count the number of unique values
one_uni_col=uni_col[uni_col==1].index #columns with exactly one unique value.
data = df.drop(columns=one_uni_col)
data.shape

# Identify columns with only one unique value

# Remove zero variance columns
#all the values are the same. This means there is no variability in the data for that column,
#making the variance (a measure of variability) equal to zero.
zero_variance_cols = [col for col in data.columns if data[col].var() == 0]
# zero_variance_cols
data = data.drop(columns=zero_variance_cols)
data.shape

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def count_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers_below = df[col][df[col] < lower_bound].count() #selects and count only values less than lower_boung
    outliers_above = df[col][df[col] > upper_bound].count()
    return outliers_below, outliers_above

#winsorization
def treat_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR #lower boundary for outliers.
    upper_bound = Q3 + 1.5 * IQR #upper boundary
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])

# Apply the outlier treatment to all numeric columns
numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns

for col in numeric_cols:
    outliers_below, outliers_above = count_outliers(data, col)
    print(f"{col}: {outliers_below} outliers below and {outliers_above} outliers above")

    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    data.boxplot(column=[col])
    plt.title(f"Boxplot for {col} before Outlier Treatment")

    treat_outliers(data, col)

    plt.subplot(1, 2, 2)
    data.boxplot(column=[col])
    plt.title(f"Boxplot for {col} after Outlier Treatment")
    plt.show()
    print("\n")

# Remove records if NA's are more than 5%
threshold = len(data.columns) * 0.05
data = data.dropna(thresh=threshold, axis=0)

# Remove columns if NA's are 50% or more
threshold = len(data) * 0.50
data = data.dropna(thresh=threshold, axis=1)

# Impute remaining missing values
for col in data.columns:
    if data[col].dtype in ['float64', 'int64']:
        data[col].fillna(data[col].median(), inplace=True)
    else:
        data[col].fillna(data[col].mode()[0], inplace=True)

# Check the dataset after missing value treatment
print("Shape after missing value treatment:", data.shape)

# Additional check for missing values
missing_values = data.isnull().sum()
print("Total missing values after treatment:", missing_values)

import seaborn as sns
# Calculate correlation matrix
corr_matrix = data.corr().abs()

# Display the heatmap
plt.figure(figsize=(10,10))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')
plt.show()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# features with correlation greater than 0.9
to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]

# Drop highly correlated features
data = data.drop(columns=to_drop)
print("Shape after dropping highly correlated features:", data.shape)

from statsmodels.stats.outliers_influence import variance_inflation_factor
# Calculate VIF for each feature
X = data.select_dtypes(include=['float64', 'int64'])
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]
print(vif_data)

threshold_vif=10
high_vif_cols = vif_data[vif_data["VIF"] > threshold_vif]["feature"].tolist()
print("Columns with VIF > 10:", high_vif_cols)

# Drop features with VIF > high_vif_threshold
data = data.drop(columns=high_vif_cols)
print("Shape after dropping features with high VIF:", data.shape)

"""# **Model Building**"""

from sklearn.model_selection import train_test_split

# Split the data into features and target
X = data.drop('target', axis=1)
y = data['target']
# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 20% --> testing

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix
from sklearn.metrics import roc_curve, auc, accuracy_score


# Model Evaluation
y_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {round(accuracy*100,2)}%')

print('Confusion Matrix:')
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            annot_kws={'fontsize': 15}, linewidths=0.5, linecolor='black')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# ROC Curve
y_pred_proba = rf.predict_proba(X_test)[:, 1] #Probabilities of the positive class
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

"""# **Hyperparameter Tuning**"""

from sklearn.ensemble import RandomForestClassifier
rf1 = RandomForestClassifier(
    n_estimators=200,         # Number of trees
    max_depth=20,             # Maximum depth of the tree
    min_samples_split=5,      # Minimum number of samples required to split an internal node
    min_samples_leaf=2,       # Minimum number of samples required to be at a leaf node
    max_features='sqrt',      # Number of features to consider when looking for the best split
    random_state=42           # Random seed for reproducibility
)
rf1.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix

# Model Evaluation
y_pred = rf1.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f'Accuracy: {round(accuracy*100,2)}%')

print('Confusion Matrix:')
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            annot_kws={'fontsize': 15}, linewidths=0.5, linecolor='black')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# ROC Curve
y_pred_proba = rf1.predict_proba(X_test)[:, 1] #Probabilities of the positive class
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

"""# **Decision Tree Classifier**"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()

# Train the classifier
dt.fit(X_train, y_train)

# Make predictions
y_pred = dt.predict(X_test)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {round(accuracy*100,2)}%')

y_pred_proba = dt.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

param_grid = {
    'criterion': ['gini', 'entropy'],
    'splitter': ['best', 'random'],
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': [None, 'sqrt', 'log2']
}

grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

grid_search.fit(X_train, y_train)
best_dt = grid_search.best_estimator_
y_pred = best_dt.predict(X_test)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {round(accuracy*100, 2)}%')
print(f'Best estimator: {best_dt}')

"""# **Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()

# Fitting the model to the training data
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {round(accuracy*100,2)}%')

y_pred_proba = lr.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

